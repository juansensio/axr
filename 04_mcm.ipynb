{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos Monte Carlo (MCM) són métodos de aprendizaje para estimar funciones de valor y descubrir políticas óptimas. Al contrario que en DP, MCM no necesitan un conocimiento completo del entorno ya que adquieren el conocimiento a través de la *experiencia* (la interacción entre el agente y su entorno).\n",
    "\n",
    "MCM están basados en promediar retornos al final de cada episodio para cada par acción-estado, momento en el que se estiman valores y se cambia la política. A diferencia de DP, donde calculamos la función de valor usando nuestro conocimiento del MDP, MCM *aprende* la función de valor a partir del retorno obtenido del MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo aprender la función estado-valor para una política determinada. Recordemos que el valor de un estado es su retorno esperado (recompensa total futura descontada) desde el estado hacia adelante. Una manera fácil de estimar este valor a través de la experiencia is promediar los retornos observados después de visitar un estado. Cuantas más veces se visite un estado, más se acercará el valor promediado al esperado. Esta es la idea fundamental de MCM. Las estimaciones de un estado no se llevan a cabo a partir de las de otros (a diferencia de DP), MCM no hacen *bootstrap*. Esto permite a MCM funcionar con solo un subset de los estados, estados de interés, ignorando el resto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:25:25.579142Z",
     "start_time": "2020-03-29T10:25:25.575137Z"
    }
   },
   "source": [
    "## Estimación de valores de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando no tenemos un modelo del entorno es más útil estimar la función acción-valor (valores para pares estado-acción). Si tenemos un modelo podemos determinar la mejor acción con un simple paso adelante, escogiendo aquella que de la mejor recompensa en el siguiente estado. Sin un modelo, sin embargo, necesitamos estimar de manera explícita el valor de cada acción para encontrar una política. Así pues, el objetico principal de MCM es estimar $q_*$. \n",
    "\n",
    "El problema de predicción presentado anteriormente puede extenderse a la función de acción-valor, considerando ahora la visita de estados $s$ en lo que se ha tomado la acción $a$, y promediando a partir de ahí en adelante. Un inconveniente de este método es que, ahora, muchos pares de estado-acción pueden no ser visitados (especialmente con el uso de políticas determinísitcas) dificultando la estimación del valor. Es por este motivo que tenemos que asegurarnos que nuestro agente es capaz de *explorar* el entorno además de *explotar* el conocimiento que ya tiene sobre el mismo. La solución general a este problema es el uso de políticas estocásticas con una probabilidad de seleccionar cualquier acción diferente de cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aproximar la política óptima podemos utilizar la misma solución presentada para DP. Por un lado tenemos la estimación de la función de valor, mientras que por otro mantenemos una política aproximada. La función de valor se va cambiando para aproximar cada vez mejor la función de valor de la política aproximada, mientras que la política se va mejorando con respecto a la función de valor actual.\n",
    "\n",
    "![](https://mymlpics.s3.eu-de.cloud-object-storage.appdomain.cloud/gpi.JPG)\n",
    "\n",
    "La mejora de la política se lleva a cabo tomando aquella acción que maximiza la función de valor en cada estad, de manera *greedy*\n",
    "\n",
    "\\begin{equation}\n",
    "    \\pi_{k+1}(s) = \\underset{a}{\\arg\\max}  \\, q_{\\pi_k}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "Esto nos asegura que $\\pi_{k+1}$ es mejor, o por lo menos igual, que $\\pi_k$, asegurando la convergencia del método. Es de esta manera como MCM puede encontrar políticas óptimas a partir de experiencia, sin el conocimiento explícito de las dinámicas del entorno. \n",
    "\n",
    "Tenemos dos opciones para asegurarnos que todas las acciones serán visitadas: métodos *on-policy*, que intentan mejorar la política que se está usando para tomar decisiones, y métodos *off-policy*, que mantienen una política diferente a la que se está usando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos *on-policy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre los métodos *on-policy*, el más común es el método $\\epsilon - greedy$, que se basa en escoger la acción con mayor valor estimado la mayoría de veces, pero con una probabilidad $\\epsilon > 0$ escoger otra acción de manera aleatoria. Esto implica que la nueva política asignará probabilidades $\\epsilon / |A(s)|$ a todas las acciones óptimas, y probabilidad $1 - \\epsilon + \\epsilon / |A(s)|$ al resto, donde $|A(s)|$ es el número de acciones posibles que se pueden llevar a cabo en el estado $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos *off-policy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los métodos *off-policy* manetenmos una política óptima, la política objetivo, y una segunda para generar el comportamiento exploratorio, la política de comportamiento. Estos métodos son más complicados y convergen más lentamente que los métodos *on-policy*, que suelen ser más sencillos. Aún así son métodos más potentes y flexibles.\n",
    "\n",
    "Considerando una política objetivo $\\pi$ y una política de comportamiento $b$, para estimar la función de valor de $\\pi$ con acciones tomadas siguiendo $b$ necesitamos que todas las acciones que se puedan tomar en $\\pi$ se tomen también en $b$. Esta es la hipotésis de *cobertura*.\n",
    "\n",
    "La mayoría de métodos *off-policy* se basan en el *muestreo por imporancia* para estimar los valores esperados de una distribución probabilística a partir de muestras obtenidas de otra distribución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:19:34.088788Z",
     "start_time": "2020-03-29T11:19:34.084772Z"
    }
   },
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCM aprenden funciones de valores y políticas óptimas a través de la experiencia. Esto les proporciona ventajas sobre DP: Pueden usarse para aprender el comportamiento óptimo directamente de la interacción, si un modelo de las dinámicas del entrono. Pueden aprender con simulaciones o modelos simplificados (en muchas ocasiones es más fácil simular el entorno que calcular de manera explícita las probabilidades de transiciones que requieren los métodos de DP). Pueden usarse para enfocarnos en una región de interés, representando de manera precisa un subset del conjunto de estados y acciones pudiendo descartar el resto.\n",
    "\n",
    "Para encontrar la política óptima seguimos utilizando el mismo proceso que en DP, iterando una política y una función de valor que se evaluan mutuamente la una sobre la otra convergiendo así en la política óptima. Para ello es importante mantener un agente que sea capaz de explorar de manera adecuada su entorno para descubrir las mejores acciones. Para ello tenemos dos opciones: Métodos *on-policy*, en los cuales la política usada para la exploración es la misma que usamos para evaluar la función de valor y que funcionan asignado una probabilidad de escoger cualquier acción mayor que cero. Por otro lado, los métodos *off-policy* utilizan dos políticas, la política objetivo que se usa como política óptima y se aprende a partir de la política de comportamiento que aporta la exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
