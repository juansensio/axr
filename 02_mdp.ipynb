{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesos de Decision Finitos de Markov\n",
    "\n",
    "Vamos a introducir la formulación de los procesos de decision finitos de Markov (MDPs), la resolución de los cuales abarca el conjunto de alogirtmos de axr. Los MDPs describen procesos de decisión secuencial en los que las acciones tomadas en diferentes situaciones no solo influen en las recompensas inmediatas sino también las futuras. A diferencia de los problemas *bandit* en los que estimamos $q_*(a)$ para cada acción $a$, con MDPs estimamos $q_*(s,a)$ como el valor para cada par estado-acción, $s$ y $a$ respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La interface Agente-Entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de los MDPs es la representación del problema de aprendizaje a través de la interacción con el entorno para conseguir un objetivo. El *agente* es el encargado de aprender y tomar decisiones, mientras que el *entorno* es todo con lo que interacciona y sobre lo que no tiene un control directo. El entrono presenta diferentes situaciones al agente en funciones de sus acciones, y provee *recompensas* que el agente intenta mazxmizar.\n",
    "\n",
    "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "El agente interacciona con el entrono de manera secuencial en pasos discretos, $t=0,1,2,...$. Para cada instante $t$ el agente recibe una representación del estado $S_t$, el cual usa para definir una acción $A_t$ la cual produce un nuevo estado $S_{t+1}$. En ese momento el entorno emite la recompensa $R_{t+1}$. En un MDP finito, el conjunto de estados, acciones y recompensas tienen un número finito de elementos. En este caso, las variables aleatorias $R_t$ y $S_t$ tiene distribuciones de probabilidad bien definidas, y que dependen únicamente del estado anterior y la accion tomada. Estas funciones de probabilidad definen la *dinámica* del MDP.\n",
    "\n",
    "Definimos las probabilidades de transición de estado como\n",
    "\n",
    "\\begin{equation}\n",
    "    p(s'|s,a) = \\sum_r p(s',r|s,a)\n",
    "\\end{equation}\n",
    "\n",
    "donde $s'$ es el nuevo estado. Podemos encontrar las recompensa esperada para un par estado-acción con la expresión\n",
    "\n",
    "\\begin{equation}\n",
    "    r(s,a) = \\sum_r r \\sum_{s'} p(s',r|s,a)\n",
    "\\end{equation}\n",
    "\n",
    "y las recompensa esperada para tríos de estado-acción-estado siguiente con\n",
    "\n",
    "\\begin{equation}\n",
    "    r(s,a,s') = \\sum_r r \\frac{p(s',r|s,a)}{p(s'|s,a)} \n",
    "\\end{equation}\n",
    "\n",
    "Los MDPs son felxibles y abstractos, y se pueden aplicar a multitud de problemas de forma distintas. Los instantes temporales no tienen porque estar referidos a intervalos de tiempor real, sino que pueden interpretarse como etapas sucesivas arbitrarias en cualquier proceso de toma de decisión. Las acciones pueden ser desde controles tales como voltajes aplicados en motores robóticos hasta decisiones de alto nivel. Del mismo modo, los estados pueden representarse utilizando desde sensaciones básicas hasta conceptos abstractos y representaciones simbólicas.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "- Imaginemos que aplicamos axr al control de temperatura de un **bioreactor**. Las diferentes acciones podrían ser la activación de elementos de calefacción y motores para mantener una temperatura objetivo. Los estados vendrían determinados por sensores de temperatura y otros sensores además de representaciones simbólicas sobre los ingredientes químicos. Las recompensas serían las medidas del ratio al cual los productos químicos se producen en el bioreactor. Mientras que las recompensas son siempre valores numéricos escalares, las acciones y estados pueden representarse mediante listas, vectores, símbolos, etc.\n",
    "\n",
    "- Imaginemos que aplicamos axr al control de un **brazo robótico** para manipular objetos. En este caso, las acciones serían los voltajes eléctricos necesarios para mover los diferentes motores, los estados se definirían con el conjunto de ángulos y posiciones de los diferentes elementos del brazo y la recompensa sería de +1 si el objeto es cogido mientras que una recompensa negativa podría aplicarse por cada instante en el que el brazo no ha logrado coger el objeto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos y Recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el axr el objetivo de un agente está determinado por la *recompensa* que le proporciona el entorno. En cada instante, la recompensa es un número, y el agente tiene que maximizar la recompensa total recibida a largo plazo. Esta es una de las propiedades más distintivas del axr en comparación a otras técnicas de aprendizaje.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
